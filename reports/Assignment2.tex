\documentclass[12pt,a4paper]{article}
\usepackage[parfill]{parskip}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{siunitx}
\bibliographystyle{plain}


\begin{document}

\centerline{\large NLP Assignment 2}
\vspace{0.2in}
\centerline{\Large\bf SVM-based Sentiment Detection of Reviews}
\vspace{0.1in}
\centerline{\large {Anik Roy, Christ's (ar899)}}
\vspace{0.1in}
\centerline{\large {\today}}
\vspace{0.05in}
\centerline{Word Count: 999\footnote{Using texcount}}
\vspace{0.2in}


\begin{multicols}{2}
  
\section{Introduction}

Support vector machines are an ML model which can be used to classify vectors in a vector space. This method can be applied to the task of classifying documents by representing each document as a vector. In this report, we choose to use a neural model, doc2vec, introduced by Mikolov and Le \cite{}, in order to generate vectors. We also [TODO] to show that the vector space produced by the doc2vec model is meaningful, by [TODO]

\section{Background}

\subsection{Support Vector Machines}

\subsection{Doc2Vec}



\section{Method}

The doc2vec implementation used was gensim \cite{gensim}.

I train the doc2vec model on 100,000 movie reviews from the Stanford Large Movie Review Dataset \cite{maas-EtAl:2011:ACL-HLT2011}. The SVM classifier is then trained on the documents found in the dataset used by pang et al. \cite{pang2002thumbs}. To tune parameters for the doc2vec model, I use a 10\% validation set to evaluate different models, then report accuracies using 10-fold cross validation over the remaining 90\%. I employed a na√Øve search strategy, starting from the parameters used by Lau and Baldwin \cite{lau2016empirical}.



I compare the doc2vec based svm model to two baseline models, naive bayes and svm, which both use a bag of n-grams representation.

\section{Results}
\blindtext
\section{Discussion}
\blindtext
\section{Conclusion}
\blindtext

\end{multicols}

\clearpage

\bibliography{refs}


\end{document}
