{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svmlight\n",
    "from typing import List, Dict\n",
    "import math\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(path):\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = list(map(lambda w: w.replace('\\n',''), lines))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(files):\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    for file in files:\n",
    "        words = load_words(file.path)\n",
    "        word_counts.update(words)\n",
    "    return word_counts\n",
    "import scandir as os\n",
    "negative_word_counts = get_word_counts(os.scandir(\"./NEG\"))\n",
    "positive_word_counts = get_word_counts(os.scandir(\"./POS\"))\n",
    "all_word_counts = negative_word_counts + positive_word_counts\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate smoothed log probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-765737fa41a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mget_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_word_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_word_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-765737fa41a2>\u001b[0m in \u001b[0;36mget_log_probs\u001b[0;34m(positive_word_counts, negative_word_counts)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_word_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         word_prob = {\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_word_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_pos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotal_unique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;34m\"NEG\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_word_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_neg\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotal_unique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         }\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "def get_log_probs(positive_word_counts, negative_word_counts):\n",
    "    total_pos = sum(positive_word_counts.values())\n",
    "    total_neg = sum(negative_word_counts.values())\n",
    "    total_unique_words = sum(all_word_counts.values())\n",
    "    # {word: {sentiment: log_prob}}\n",
    "    probs = {}\n",
    "\n",
    "    for word, count in all_word_counts.items():\n",
    "        word_prob = {\n",
    "            \"POS\": math.log((positive_word_counts[word]+1)/(total_pos+total_unique_words)),\n",
    "            \"NEG\": math.log((negative_word_counts[word]+1)/(total_neg+total_unique_words))\n",
    "        }\n",
    "        probs[word] = word_prob\n",
    "    return probs\n",
    "get_log_probs(positive_word_counts, negative_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_log_class_probs(posdir, negdir):\n",
    "    pos_docs = len(os.listdir(posdir))\n",
    "    neg_docs = len(os.listdir(negdir))\n",
    "    total = pos_docs + neg_docs\n",
    "    return {\n",
    "        \"POS\": math.log(pos_docs/total),\n",
    "        \"NEG\": math.log(neg_docs/total)\n",
    "    }    \n",
    "get_log_class_probs(posdir=\"./POS\", negdir=\"./NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classify_file(filename, class_probs, word_log_probs):\n",
    "    posSum = 0\n",
    "    negSum = 0\n",
    "    words = load_words(filename)\n",
    "    for word in words:\n",
    "        if word in word_log_probs:\n",
    "            posSum += word_log_probs[word][\"POS\"]\n",
    "            negSum += word_log_probs[word][\"NEG\"]\n",
    "    posSum += class_probs[\"POS\"]\n",
    "    negSum += class_probs[\"NEG\"]\n",
    "    return \"POS\" if (posSum > negSum) else \"NEG\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classify_files(files, class_probs, word_log_probs):\n",
    "    return {file : naive_bayes_file(file) for file in files}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = load_files(container_path=\".\", categories=[\"POS\", \"NEG\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./POS/cv132_5618.tag'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def substr(s, i,j):\n",
    "    return s[i:j]\n",
    "np.argwhere(reviews.filenames != './NEG/cv000')\n",
    "reviews.filenames[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = \\\n",
    "train_test_split(reviews.data, reviews.target, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams(feature_train, label_train, feature_test, label_test):\n",
    "    # Train\n",
    "    cv = CountVectorizer(stop_words=None)\n",
    "    feature_train_counts = cv.fit_transform(feature_train)\n",
    "    nb_classifier = MultinomialNB(alpha=1).fit(feature_train_counts, label_train)\n",
    "    # Evaluate\n",
    "\n",
    "    X = cv.transform(feature_test)\n",
    "    predicted = nb_classifier.predict(X)\n",
    "    return np.mean(predicted == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_train_and_evaluate(feature_train, label_train, feature_test, label_test):\n",
    "    # Train\n",
    "    cv = CountVectorizer(stop_words=None)\n",
    "    feature_train_counts = cv.fit_transform(feature_train)\n",
    "    nb_classifier = MultinomialNB(alpha=1).fit(feature_train_counts, label_train)\n",
    "    # Evaluate\n",
    "\n",
    "    X = cv.transform(feature_test)\n",
    "    predicted = nb_classifier.predict(X)\n",
    "    return np.mean(predicted == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_train_and_evaluate(feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(feature_train, label_train, feature_test, label_test):\n",
    "    # Train\n",
    "    cv = CountVectorizer(ngram_range=(2,2), stop_words=None)\n",
    "    feature_train_bigram_counts = cv.fit_transform(feature_train)\n",
    "    nb_classifier = MultinomialNB(alpha=1).fit(feature_train_bigram_counts, label_train)\n",
    "\n",
    "    # Evaluate\n",
    "    X = cv.transform(feature_test)\n",
    "    predicted = nb_classifier.predict(X)\n",
    "    return np.mean(predicted == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams(feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming + Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def stemming_unigrams(feature_train, label_train, feature_test, label_test):\n",
    "    #Train\n",
    "    cv = CountVectorizer(ngram_range=(1,1), stop_words=None, tokenizer=stemming_tokenizer)\n",
    "    feature_train_bigram_counts = cv.fit_transform(feature_train)\n",
    "    nb_classifier = MultinomialNB(alpha=1).fit(feature_train_bigram_counts, label_train)\n",
    "\n",
    "    # Evaluate\n",
    "    X = cv.transform(feature_test)\n",
    "    predicted = nb_classifier.predict(X)\n",
    "    return np.mean(predicted == label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_unigrams(feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "def cross_validate_nb(reviews, func):\n",
    "    splits = skf.split(reviews.data, reviews.target)\n",
    "    scores = []\n",
    "    for train_index, test_index in splits:\n",
    "        X_train, X_test = np.asarray(reviews.data)[train_index], np.asarray(reviews.data)[test_index]\n",
    "        y_train, y_test = reviews.target[train_index], reviews.target[test_index]\n",
    "        scores.append(func(X_train, y_train, X_test, y_test))\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_nb(reviews, unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_nb(reviews, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_nb(reviews, stemming_unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import svmlight\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "reviews = load_files(container_path=\".\", categories=[\"POS\", \"NEG\"])\n",
    "reviews.data\n",
    "reviews.filenames\n",
    "feature_train, feature_test, label_train, label_test = \\\n",
    "train_test_split(reviews.data, reviews.target, test_size=0.1)\n",
    "\n",
    "cv = CountVectorizer(stop_words=None)\n",
    "feature_train_counts = cv.fit_transform(feature_train)\n",
    "label_train[label_train==0]=-1\n",
    "dump_svmlight_file(feature_train_counts, label_train, \"svm_out\", zero_based=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call([\"svm_learn\", \"svm_out\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test_counts = cv.transform(feature_test)\n",
    "label_test[label_test==0]=-1\n",
    "\n",
    "dump_svmlight_file(feature_test_counts, label_test, \"svm_test_out\", zero_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call([\"svm_classify\", \"svm_test_out\",\"model\", \"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98287322\r\n",
      "0.074604071\r\n",
      "-0.57099994\r\n",
      "-2.2587009\r\n",
      "-0.29174799\r\n",
      "5.377817\r\n",
      "-0.091312796\r\n",
      "2.2776926\r\n",
      "-0.30844986\r\n",
      "-0.30637832\r\n",
      "-0.64817363\r\n",
      "-0.6400294\r\n",
      "-0.52056378\r\n",
      "-0.3128901\r\n",
      "-1.1536328\r\n",
      "0.62604578\r\n",
      "-1.1160028\r\n",
      "2.689419\r\n",
      "0.16954575\r\n",
      "-0.085207517\r\n",
      "-0.58443436\r\n",
      "-0.28622555\r\n",
      "-1.0793179\r\n",
      "1.2037501\r\n",
      "2.093639\r\n",
      "-0.21668892\r\n",
      "-0.83525818\r\n",
      "-0.6233429\r\n",
      "-1.0580845\r\n",
      "-0.050027912\r\n",
      "-0.24510117\r\n",
      "-1.6731023\r\n",
      "-1.0322898\r\n",
      "-0.86676932\r\n",
      "0.072943901\r\n",
      "0.033111139\r\n",
      "1.2852925\r\n",
      "0.092368036\r\n",
      "-0.053854434\r\n",
      "-0.83750909\r\n",
      "-0.46447945\r\n",
      "-0.096754284\r\n",
      "-0.74283587\r\n",
      "0.15314441\r\n",
      "-0.5117\r\n",
      "-0.001580273\r\n",
      "0.24633156\r\n",
      "0.33567507\r\n",
      "1.4891685\r\n",
      "-0.47314745\r\n",
      "-0.60390055\r\n",
      "0.60901112\r\n",
      "-0.90891548\r\n",
      "0.145736\r\n",
      "0.30570762\r\n",
      "0.49148189\r\n",
      "-0.30791946\r\n",
      "0.39665084\r\n",
      "2.0180885\r\n",
      "1.3799447\r\n",
      "-0.49014522\r\n",
      "0.55708774\r\n",
      "-0.34992737\r\n",
      "3.114812\r\n",
      "0.21280077\r\n",
      "-0.8389494\r\n",
      "-0.068284577\r\n",
      "-0.4080433\r\n",
      "-1.3044649\r\n",
      "0.026515026\r\n",
      "-0.81939518\r\n",
      "-0.56585431\r\n",
      "-1.1041063\r\n",
      "0.74567713\r\n",
      "0.28800595\r\n",
      "-1.7545375\r\n",
      "-0.95639584\r\n",
      "-0.24865282\r\n",
      "-0.32979116\r\n",
      "-0.033853944\r\n",
      "0.029821385\r\n",
      "-0.12586297\r\n",
      "-0.68539158\r\n",
      "-1.5210241\r\n",
      "-0.15528348\r\n",
      "-0.033437881\r\n",
      "-0.63362362\r\n",
      "-0.32790051\r\n",
      "0.16838299\r\n",
      "-0.60616767\r\n",
      "-0.45711971\r\n",
      "0.44309486\r\n",
      "0.19785888\r\n",
      "-0.91549417\r\n",
      "0.064328041\r\n",
      "0.19131294\r\n",
      "-0.52037323\r\n",
      "-1.7168887\r\n",
      "-1.6903012\r\n",
      "-0.51197552\r\n",
      "-0.38937508\r\n",
      "-0.60087163\r\n",
      "-0.56933442\r\n",
      "-1.3547126\r\n",
      "-0.7190535\r\n",
      "1.6183192\r\n",
      "-0.67899283\r\n",
      "1.512593\r\n",
      "-0.47642348\r\n",
      "0.80911977\r\n",
      "1.1434634\r\n",
      "-0.32125472\r\n",
      "-1.1513921\r\n",
      "0.69402428\r\n",
      "-1.3065859\r\n",
      "0.88288511\r\n",
      "-0.41836708\r\n",
      "-0.76905585\r\n",
      "-1.1605493\r\n",
      "0.55417954\r\n",
      "1.6613443\r\n",
      "0.4272571\r\n",
      "-0.11518738\r\n",
      "-0.91524317\r\n",
      "-1.6145672\r\n",
      "-0.56551542\r\n",
      "0.4853833\r\n",
      "-0.31902228\r\n",
      "0.89258585\r\n",
      "1.2409467\r\n",
      "-0.50888567\r\n",
      "0.32195204\r\n",
      "0.10548384\r\n",
      "-0.25832809\r\n",
      "-0.23142372\r\n",
      "-0.36946445\r\n",
      "-0.11280902\r\n",
      "-1.3148777\r\n",
      "0.90521598\r\n",
      "0.50333659\r\n",
      "-0.41584441\r\n",
      "-0.13825013\r\n",
      "-1.1120928\r\n",
      "1.1198328\r\n",
      "0.92172427\r\n",
      "-1.0264557\r\n",
      "-0.51897715\r\n",
      "0.094905768\r\n",
      "0.8860736\r\n",
      "0.48048665\r\n",
      "0.94389767\r\n",
      "0.80175428\r\n",
      "-1.070762\r\n",
      "-1.0538969\r\n",
      "-0.97873244\r\n",
      "-0.25104454\r\n",
      "0.0944473\r\n",
      "-0.74150037\r\n",
      "0.15811312\r\n",
      "-0.48171446\r\n",
      "0.33855389\r\n",
      "1.0968506\r\n",
      "0.90359896\r\n",
      "-0.5152048\r\n",
      "-0.60566315\r\n",
      "-0.79554916\r\n",
      "-0.85212329\r\n",
      "0.80150121\r\n",
      "1.3731653\r\n",
      "-0.3312961\r\n",
      "-0.74890249\r\n",
      "-0.39304033\r\n",
      "-0.11721362\r\n",
      "-0.6293779\r\n",
      "1.0654691\r\n",
      "0.11881471\r\n",
      "2.0261937\r\n",
      "1.6944824\r\n",
      "-0.22435502\r\n",
      "-0.091231212\r\n",
      "0.082479949\r\n",
      "-0.77882358\r\n",
      "-0.9477381\r\n",
      "0.055196521\r\n",
      "-0.89986613\r\n",
      "0.82608359\r\n",
      "-0.98501464\r\n",
      "-0.96225974\r\n",
      "-1.0042719\r\n",
      "-0.86040849\r\n",
      "-0.65097345\r\n",
      "-0.54289992\r\n",
      "0.63504999\r\n",
      "-0.64498199\r\n",
      "-1.4632638\r\n",
      "-0.76983302\r\n",
      "-0.30173863\r\n",
      "-7.0764799e-05\r\n",
      "-1.0069683\r\n",
      "-0.082194745\r\n"
     ]
    }
   ],
   "source": [
    "%cat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
